
#%%
import gc
import cv2
import glob
import math
import numpy as np
import pandas as pd
from pandas.io.parsers import read_csv
import seaborn as sns
import matplotlib.pyplot as plt

from skimage import io

from fast_slic.avx2 import SlicAvx2


images = glob.glob("IMAGES FILEPATH")
images.sort()

#%%
#Label names
labels = ('Sed_dropst','Rubble_cor', 'D_coral' ,'LICO')

NO_LABEL = 255

all_labels = glob.glob("ALL LABELS FILEPATH")
all_labels.sort()

def colorize_prediction(mask, labels):

    '''
    Simple function to colorize the mask
    mask   -> the mask created by fast_mss
    labels -> all possible labels in dataset
    '''
    # Creates a color palette
    cp = sns.color_palette("hls", len(labels))
   
    colored_mask = np.zeros(shape = (mask.shape[0], mask.shape[1], 3))

    # recolors all labels 
    for _ in np.unique(mask):
           
            colored_mask[mask == _] = cp[_]
        
    # returns a numpy array, (H, W, 3), [0, 1]
    return colored_mask

def display(a, b):

    '''
    A simple display function that shows the image and mask
    a & b  -> the actual image, the ground-truth dense annotations or mask
    '''

    plt.figure(figsize = (10, 10))

    plt.subplot(1, 2, 1)
    plt.imshow(a)
    
    plt.subplot(1, 2, 2)
    plt.imshow(b)

    plt.show()

def compute_segmentations(start_iter, end_iter, num_iter):
    '''
    From Alonso et al. 2019 (CoralSeg)
    This function calculates how many superpixels should be formed during each
    iteration of Fast-MSS, and is dependent on parameters required from the user
    start_iter -> the larger value (1,000 - 10,000)
    end_iter   -> the smaller value (10 -  1,000
    num_iter   -> number of total iterations; more tends to be better, but takes longer (20 - 50)
    '''
    # value for determining how much to decrease by each iteration
    reduction_factor = math.pow(float(end_iter) / start_iter, 1. / (num_iter - 1))

    # returns a list containing values in descending order
    return [int(start_iter * math.pow(reduction_factor, iters)) for iters in np.arange(num_iter)]

def mode(a, null_value = NO_LABEL, axis = 0):

    '''
    Scipy's code to calculate the statistical mode of an array
    Here we include the ability to input a NULL value that should be ignored
    So in no situation should an index in the resulting dense annotations contain
    the background/NULL value.
    a -> the stack containing multiple 2-d arrays with the same dimensions
    '''

    a = np.array(a)
    scores = np.unique(np.ravel(a))  
    testshape = list(a.shape)       
    testshape[axis] = 1
    oldmostfreq = np.zeros(testshape, dtype = int)    
    oldcounts = np.zeros(testshape, dtype = int)       

    for score in scores:

        # if the mode is a null value,
        # use the second most common value instead
        if(score == null_value):
            continue

        template = (a == score)                                 
        counts = np.expand_dims(np.sum(template, axis), axis)

        mostfrequent = np.where(counts > oldcounts, score, oldmostfreq)  
        oldcounts = np.maximum(counts, oldcounts)
        oldmostfreq = mostfrequent

    return mostfrequent[0]

def fast_mss(image, sparse, labels, start_iter, end_iter, num_iter, method = 'mode'):

    '''
    The function used to create dense annotations from sparse. Requires the installation
    of Fast-SLIC (could plug in other over-segmentation algorithsm quite easily though).
    Takes in an image and the sparse annotations, and over multiple iterations segments
    the image into some number of superpixels; during each iteration sparse labels are
    propagated to pixel associated with a superpixel that the label in located within.
    This occurs over multiple iterations where the number of superpixels changes from
    high (small superpixels) to low (large superpixels). Once these are made 
    (H, W, num_iterations), they are combined by a 'join' or by calculating the 'mode' 
    across the 3rd dimension. The final result is a set of dense annotations for the 
    image.
    image           --> the input image, (smaller makes it go faster obvs)
    sparse          --> a set of sparse labels for image, pandas df with columns [X, Y, Label]
    labels          --> a list containing all possible labels in dataset
    start_iter      --> larger number denoting how many superpixels should be formed in start 
    end_iter        --> smaller number denoting how many superpixels should be formed in end 
    num_iter        --> total number of iterations; higher makes better results, but takes longer
    method          --> the method of combining labels from each iteration; 'join' or 'mode'
    '''

    sparse = pd.read_csv(sparse)
    # stores all masks from each iteration, 
    # how many superpixels to be formed each iteration
    all_masks = []
    segmentations = compute_segmentations(start_iter, end_iter, num_iter)

    # Loops through, each time a mask is created with different settings, and is accumulated
    for _ in range(num_iter):
        
        # number of superpixels this iteration
        n_segments = segmentations[_]
    
        # Uses CPU to create segmented image with current params
        slic = SlicAvx2(num_components = n_segments, compactness = 25)
        segmented_image = slic.iterate(cv2.cvtColor(image, cv2.COLOR_RGB2LAB))

        # The XY location of each annotation, along with the label
        X = np.array(sparse['X'].values, dtype = 'uint16')
        Y = np.array(sparse['Y'].values, dtype = 'uint16')
        L = np.array(sparse['Label'].values, dtype = 'str')  

        # By plugging in the annotation locations into the segmented image
        # you get all of the segment IDs that contain annotations (desired segments, DS)
        # Then for each DS, find the class label for the annotations within it (color label, CL)
        DS = segmented_image[Y, X]                               
        CL = np.array([labels.index(L[i]) for i in range(len(DS))])
        
        # If multiple annotations are within the same segment, find
        # the most common label among those annotations and provide it
        # as the final label for that segment (majority rule)
        if(len(DS) != len(np.unique(DS))):

            for segment_ID in np.unique(DS):
                annotations_in_segment = list(np.where(DS == segment_ID)[0])
                labels_of_annotations = [CL[a] for a in annotations_in_segment]
                most_common_label_in = max(set(labels_of_annotations), key = labels_of_annotations.count)
                CL[annotations_in_segment] = most_common_label_in
        
        # Lastly, reform them as a dictionary to speed up the the next process by 50%
        pairs = dict(zip(DS, CL))

        # temporary mask that holds the labels for each superpixel during this iteration
        mask = np.full(shape = image.shape[0:2], fill_value = NO_LABEL)

        # Loops through values in segmented mask (as a dict), gets labels, stores in 2D array
        for index, segVal in enumerate(list(pairs)):
            mask[segmented_image == segVal] = pairs[list(pairs)[index]]
            # provides each individual pixel with the class label of the superpixel

        # Collects all masks made of this image for all iterations
        all_masks.append(mask)
        
        # helps stave of out of memory errors
        # for a large number of images of high resolution
        # may want to clean memory often, restart kernel, or 
        # run from command line using a bash script
        gc.collect()
                    
    # Now that the loop is over, we have a bunch of segmented images where there are
    # pixels with class labels, and pixels with no label (255). We can combine them
    # following Alonso et al. 2019's join method, or Pierce et al. 2020's mode method
    
    # Starting with the first mask which is the most detailed, we mask it with less a
    # detailed mask that contains more labeled pixels, mask_b. With each iteration, 
    # the class mask (final_mask) fills up with the additional pixels provided by mask_b; 
    # no pixels get replaced by mask_b, they only add to final_mask
    if(method == 'join'):
        
        # First mask is most detailed, start with it
        final_mask = all_masks[0]

        # Then go through the remaining masks
        for _ in all_masks[1:]:
            mask_b = _

            # find the pixels in mask_a that are not labelled, assign them with the labels
            # of those same pixels in mask_b
            final_mask[final_mask == NO_LABEL] = mask_b[final_mask == NO_LABEL] 
            
    # Jordan's method, 'mode'
    else:
        # Returns a 2-d array that matches the size of the original image
        # the mode across the 0-th axis
        final_mask = mode(all_masks)

    return final_mask

#i = io.imread(images[0])
#j = all_labels[0]
#mask = fast_mss(i, j, labels, start_iter= 1000, end_iter= 10, num_iter = 20, method= 'mode')
#io.imsave(arr = mask , fname = 'C:/Users/David/Desktop/Masters/Thesis/Sample_Piddington/Masks/Mask1.jpg')

#%%
for i,j in zip(images, all_labels):
    img = io.imread(i)
    mask = fast_mss(img, j, labels, start_iter= 7500, end_iter= 100, num_iter = 20, method= 'mode')
    #mask_rgb = colorize_prediction(mask, labels)
    io.imsave(arr = mask, fname = j.replace('ALL LABELS FILENAME', 'MASKS FILENAME') + 'MSS.png')
    #display(img, colorize_prediction(mask, labels))
    


